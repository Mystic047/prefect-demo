# Prefect Production Setup

Clean production-ready setup for Prefect data orchestration with SQL Server and external PostgreSQL.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Docker Containers                 â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Prefect    â”‚    â”‚   Prefect    â”‚      â”‚
â”‚  â”‚   Server     â”‚â—„â”€â”€â”€â”¤   Worker     â”‚      â”‚
â”‚  â”‚  (Port 4200) â”‚    â”‚ (chula-pool) â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚
           â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PostgreSQL  â”‚    â”‚ SQL Server  â”‚
    â”‚   (AWS)     â”‚    â”‚   (Azure)   â”‚
    â”‚             â”‚    â”‚             â”‚
    â”‚ Prefect     â”‚    â”‚ Your Data   â”‚
    â”‚  Metadata   â”‚    â”‚   Source    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Folder Structure

```
prefect-demo/
â”œâ”€â”€ flows/                  # Flow definitions
â”‚   â””â”€â”€ chula_extraction/   # Chula extraction flow package
â”œâ”€â”€ database_connection/    # SQL Server connection helpers
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â””â”€â”€ init_prefect.sh     # Database setup script (optional)
â”œâ”€â”€ output/                 # Flow outputs (JSON files)
â”œâ”€â”€ docker-compose.yml      # Container orchestration
â”œâ”€â”€ Dockerfile              # Worker container image
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.template           # Environment variables template
â”œâ”€â”€ how_to_deploy.md        # Step-by-step deploy/run commands
â””â”€â”€ README.md               # This file
```

## Prerequisites

### 1. PostgreSQL Database (AWS RDS)

Create a PostgreSQL database for Prefect metadata:

```sql
CREATE DATABASE prefect_db;
CREATE USER prefect_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE prefect_db TO prefect_user;
```

### 2. SQL Server Database

Ensure your SQL Server is accessible and you have:
- Host/endpoint
- Port (default: 1433)
- Database name
- Username and password

## Setup Instructions

### Step 1: Configure Environment

```bash
# Copy template
cp .env.template .env

# Edit with your credentials
nano .env
```

Update these values:
```env
# PostgreSQL (Prefect metadata storage)
PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://prefect_user:your_password@your-rds-host.amazonaws.com:5432/prefect_db

# SQL Server (Your data source)
SQLSERVER_HOST=your-server.database.windows.net
SQLSERVER_PORT=1433
SQLSERVER_DATABASE=your_database
SQLSERVER_USER=your_username
SQLSERVER_PASSWORD=your_password
```

### Step 2: Start Services

```bash
docker-compose up -d
```

Verify containers are running:
```bash
docker-compose ps
```

### Step 3: ğŸš€ Deploy and Run Flows

You now deploy flows directly from your Windows host using the Prefect CLI (no `deployments` folder). See `how_to_deploy.md` for detailed, copy-paste commands.

Basic pattern from Windows (Git Bash or CMD):

```bash
export PREFECT_API_URL=http://localhost:4200/api

python -m prefect deploy flows/chula_extraction/flow_chula_extract_data.py:extract_chula_data \
  --name person-extraction \
  --pool chula-pool \
  --params '{"table_name":"Person","output_filename":"Person.json","limit":1000}'

python -m prefect deployment run 'extract_chula_data/person-extraction'
```

## Flow Parameters

The main extraction flow `extract_chula_data` accepts:

- `table_name` (required): SQL Server table to extract
- `output_filename` (optional): Output JSON filename
- `limit` (optional): Max rows to extract (default: 1000)

## Output Files

Extracted data is saved to `./output/` as JSON files:
```
output/
â”œâ”€â”€ Person.json
â”œâ”€â”€ Dept.json
â””â”€â”€ EQ.json
```

## Monitoring

Access Prefect UI: http://localhost:4200

- View flow runs
- Monitor logs
- Check work pool status
- Manage deployments

## Common Commands

```bash
# View logs
docker-compose logs -f prefect-worker

# Stop services
docker-compose down

# Restart worker
docker-compose restart prefect-worker

# Check work pools
docker exec prefect-server prefect work-pool ls
```

## ğŸ“¦ Deployments Overview

You create and run deployments from your Windows host using the Prefect CLI.

ğŸ§ª Basic pattern:

```bash
export PREFECT_API_URL=http://localhost:4200/api

python -m prefect deploy flows/chula_extraction/flow_chula_extract_data.py:extract_chula_data \
  --name person-extraction \
  --pool chula-pool \
  --params '{"table_name":"Person","output_filename":"Person.json","limit":1000}'

python -m prefect deployment run 'extract_chula_data/person-extraction'
```

For more deployment examples (Dept, EQ, Craft, scheduling, deleting deployments), see `how_to_deploy.md`.

**Disable Prefect's default git pull step**

- After each deploy, open Prefect UI â†’ Deployments â†’ select your deployment â†’ **Edit** â†’ Pull steps.
- Remove the auto-generated `prefect.deployments.steps.git_clone` step.
- Add a `prefect.deployments.steps.set_working_directory` step that points at `/app` (code is already there via Docker volume mounts).
- Save the deployment; future runs will skip git entirely and use the mounted files.

### ğŸ§© How Names Fit Together

When you deploy, three related pieces come together:

```text
flows/chula_extraction/flow_chula_extract_data.py
â””â”€ Python file that defines the flow function

extract_chula_data
â””â”€ Flow function name (also used in @flow(name="extract_chula_data"))

extract_chula_data/person-extraction
â””â”€ Deployment name in Prefect:  <flow name>/<deployment name>
```

ğŸ—‘ Example delete command (old naming style):

```bash
python -m prefect deployment delete 'chula-extract-sqlserver-data/person-extraction'
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           Flow name / deployment name
```

With the new naming, the same pattern applies, just with `extract_chula_data` as the flow name:

```bash
python -m prefect deployment delete 'extract_chula_data/person-extraction'
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    Flow name / deployment name
```bash
# ğŸ“œ View logs
docker-compose logs -f prefect-worker

# â¹ Stop services
docker-compose down

# ğŸ”„ Restart worker
docker-compose restart prefect-worker

# ğŸ§º Check work pools
docker exec prefect-server prefect work-pool ls
```
No manual table creation needed!

## Troubleshooting

### Cannot connect to PostgreSQL

```bash
# Test connection
docker run --rm postgres:15-alpine psql "postgresql://user:pass@host:5432/db" -c "SELECT 1"
```

### SQL Server connection fails

Verify:
- Firewall rules allow connections
- ODBC Driver 17 is installed (included in Dockerfile)
- Connection string format is correct

### Work pool not found

```bash
# Create manually
docker exec prefect-server prefect work-pool create chula-pool --type process
```

## Production Checklist

- [ ] PostgreSQL database created on AWS RDS
- [ ] SQL Server accessible from Docker network
- [ ] `.env` file configured with real credentials
- [ ] Database migrations completed
- [ ] Work pool `chula-pool` created
- [ ] Flow deployed successfully
- [ ] Test run completed
- [ ] Monitoring in Prefect UI working

## Security Notes

- Never commit `.env` file
- Use strong passwords for database connections
- Restrict PostgreSQL access to known IPs
- Use SSL/TLS for database connections in production
- Consider using secrets management (AWS Secrets Manager, Azure Key Vault)
